{"paragraphs":[{"text":"%pyspark\n\n###############################################\n#   Importando pacotes & definindo funções    #\n###############################################\n\nfrom datetime import *\nfrom pyspark import StorageLevel\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# Função para criptografar colunas de um dataframe\ndef criptografa_df(dataframe, colunas_a_criptografar):\n    colunas = []\n    \n    for coluna in dataframe.columns:\n        if coluna in colunas_a_criptografar:\n            aux = sha2(col(coluna), 0).alias(coluna)\n        else:\n            aux = coluna\n        colunas.append(aux)\n    return colunas\n\n###############################\n#   Definição de variáveis    #\n###############################\n\n# Lista de colunas a serem criptografadas\ncripto_ls = [\"cpf\", \"customer_name\", \"delivery_address_city\", \"delivery_address_district\", \"delivery_address_country\", \"delivery_address_district\", \n    \"delivery_address_latitude\", \"delivery_address_longitude\", \"delivery_address_state\", \"delivery_address_zip_code\", \"customer_phone_area\", \n    \"customer_phone_number\", \"merchant_zip_code\"]\n\n# Ordem das colunas na base final\ncolunas = [\"order_id\", \"customer_id\", expr(\"cpf as customer_cpf\"), \"customer_name\", \"customer_phone_area\", \"customer_phone_number\", expr(\"language as customer_language\"), \n    \"costumer_created_at\", expr(\"active as is_costumer_active\"), \"merchant_id\", \"merchant_created_at\", expr(\"enabled as is_merchant_enabled\"), \n    expr(\"price_range as merchant_price_range\"), expr(\"average_ticket as merchant_average_ticket\"), expr(\"takeout_time as merchant_takeout_time\"), \n    expr(\"delivery_time as merchant_delivery_time\"), expr(\"minimum_order_value as merchant_minimum_order_value\"), \"merchant_zip_code\", \"merchant_city\",\n    \"merchant_state\", \"merchant_country\", \"merchant_latitude\", \"merchant_longitude\", \"merchant_timezone\", \"delivery_address_city\", \n    \"delivery_address_country\", \"delivery_address_district\", \"delivery_address_external_id\", \"delivery_address_latitude\", \"delivery_address_longitude\", \n    \"delivery_address_state\", \"delivery_address_zip_code\", expr(\"items as order_items\"), \"order_created_at\", expr(\"order_scheduled as is_order_scheduled\"),\n    \"order_scheduled_date\", \"order_total_amount\", \"origin_platform\", \"order_status\"]\n\n# Caminho de origem da raw order\norigem_order = \"s3://ifood-raw-order/\"\n\n# Caminho de origem da raw order status\norigem_status = \"s3://ifood-raw-order-status/\"\n\n# Caminho de origem da raw restaurant\norigem_rest = \"s3://ifood-raw-restaurant/\"\n\n# Caminho de origem da raw consumer\norigem_cons = \"s3://ifood-raw-consumer/\"\n\n# Caminho de destino da trusted order\ndestino_trusted = \"s3://ifood-trusted-order/\"\n\n# Inicia sessão spark\nspark = SparkSession.builder.appName(\"ifood-trusted-order-dev\").getOrCreate()\n\n# Configurações básicas para o spark\nspark.conf.set(\"spark.sql.maxPartitionBytes\", 200 * 1024 * 1024) # Seta a quantidade máxima de bytes em uma partição ao ler os arquivos de entrada (Entre 100MB e 200MB é o ideal)\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"DYNAMIC\") # Necessário para sobrescrever partições\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 100 * 1024 * 1024) # Define o limiar para broadcasting de até 100MB","user":"anonymous","dateUpdated":"2019-12-23T22:43:47+0000","config":{"lineNumbers":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577135879795_51819919","id":"20191223-114810_835126986","dateCreated":"2019-12-23T21:17:59+0000","dateStarted":"2019-12-23T22:43:47+0000","dateFinished":"2019-12-23T22:44:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:172"},{"title":"Lê as origens order e order-status","text":"%pyspark\n\n# Lê a raw order\norderDF = spark.read.parquet(origem_order).filter(col(\"dt\") < \"2019-01-31\").drop(\"dt\", \"dt_proc\").persist(StorageLevel.DISK_ONLY)\nprint(\"> Volumetria da raw order: {}\".format(orderDF.count()))\n\n# Lê a raw order status\nstatusDF = spark.read.parquet(origem_status).filter(col(\"dt\") < \"2019-01-31\").drop(\"dt\", \"dt_proc\").persist(StorageLevel.DISK_ONLY)\nprint(\"> Volumetria da raw order status: {}\".format(statusDF.count()))\n\n# Lê a raw restaurant\nrestDF = spark.read.parquet(origem_rest).drop(\"dt\", \"dt_proc\").persist(StorageLevel.DISK_ONLY)\nprint(\"> Volumetria da raw restaurant: {}\".format(restDF.count()))\n\n# Lê a raw consumer\nconsDF = spark.read.parquet(origem_cons).drop(\"dt\", \"dt_proc\").persist(StorageLevel.DISK_ONLY)\nprint(\"> Volumetria da raw consumer: {}\".format(consDF.count()))","user":"anonymous","dateUpdated":"2019-12-23T22:43:51+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"> Volumetria da raw order: 3622900\n> Volumetria da raw order status: 7222896\n> Volumetria da raw restaurant: 7292\n> Volumetria da raw consumer: 809323\n"}]},"apps":[],"jobName":"paragraph_1577135879799_1384846151","id":"20191223-120737_447564544","dateCreated":"2019-12-23T21:17:59+0000","dateStarted":"2019-12-23T22:44:16+0000","dateFinished":"2019-12-23T22:45:00+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:173"},{"title":"Pegamos apenas os status mais recentes de cada pedido","text":"%pyspark\n\n# Status mais recentes de cada pedido\nstatusDF_ = statusDF \\\n    .withColumn(\"order\", expr(\"row_number() over(partition by order_id order by order_id, created_at desc)\")) \\\n    .filter(col(\"order\") == 1) \\\n    .select(\"order_id\", \"created_at\", expr(\"value as order_status\"))\n    \nstatusDF_.show(5, False)","user":"anonymous","dateUpdated":"2019-12-23T23:19:27+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------------------------------+------------------------+------------+\n|order_id                            |created_at              |order_status|\n+------------------------------------+------------------------+------------+\n|0002fe02-d7dc-4232-b7ac-3394019ce240|2019-01-25T01:05:07.000Z|CONCLUDED   |\n|000cef8c-83c7-49eb-a0fb-404e6dc2150e|2019-01-18T00:45:02.000Z|CONCLUDED   |\n|0010995b-9212-455a-85ea-11ea7dd526c1|2019-01-02T00:15:14.000Z|CONCLUDED   |\n|0012d95c-9c4b-4244-86b5-dcf87677dcc1|2019-01-03T20:15:06.000Z|CONCLUDED   |\n|0013fc5c-4c10-4402-886c-1b8166e4632e|2019-01-06T16:20:27.000Z|CONCLUDED   |\n+------------------------------------+------------------------+------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1577135879799_2004804122","id":"20191223-121509_717017178","dateCreated":"2019-12-23T21:17:59+0000","dateStarted":"2019-12-23T22:45:00+0000","dateFinished":"2019-12-23T22:45:02+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:174"},{"title":"Cruzando com a base orders","text":"%pyspark\n\n# Otimização do join\nspark.conf.set(\"spark.sql.shuffle.partitions\", int(spark.conf.get(\"spark.default.parallelism\")))\norderDF \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"order_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"orders\")\n    \norderDF_ = spark.read.table(\"orders\")\n\nstatusDF_ \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"order_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"status\")\n    \nstatusDF_ = spark.read.table(\"status\")\n    \n# Traz os status mais recentes de cada pedido\njoin00 = orderDF_.join(statusDF_, on=[\"order_id\"], how=\"left\")","user":"anonymous","dateUpdated":"2019-12-23T22:43:56+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577135879800_-1161064121","id":"20191223-122852_2071888743","dateCreated":"2019-12-23T21:17:59+0000","dateStarted":"2019-12-23T22:45:02+0000","dateFinished":"2019-12-23T22:45:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:175"},{"title":"Tratamento de duplicidade na base order","text":"%pyspark\n\n# Em desenvolvimento verificamos que a base order apresentava duplicidade devido a registros inválidos na data do pedido\n# Vamos tratá-los considerando todo registro que tiver na diferença created_at e order_created_at um valor menor ou igual a 1 como válido\njoin00_ = join00 \\\n    .withColumn(\"order_created_at\", col(\"order_created_at\").cast(TimestampType())) \\\n    .withColumn(\"created_at\", col(\"created_at\").cast(TimestampType())) \\\n    .withColumn(\"fl_valid\", expr(\"case when datediff(created_at, order_created_at) <= 1 then 1 else 0 end\")) \\\n    .filter(col(\"fl_valid\") == 1) \\\n    .drop(\"fl_valid\", \"created_at\")","user":"anonymous","dateUpdated":"2019-12-23T22:43:59+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577135879800_-845409633","id":"20191223-125627_1436993064","dateCreated":"2019-12-23T21:17:59+0000","dateStarted":"2019-12-23T22:45:36+0000","dateFinished":"2019-12-23T22:45:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:176"},{"title":"Cruzamento com as bases consumer e restaurant","text":"%pyspark\n\n# Otimização do join\njoin00_ \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"customer_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"temp00\")\n\ntemp00DF = spark.read.table(\"temp00\")\n\nconsDF \\\n    .drop(\"customer_name\") \\\n    .withColumn(\"created_at\", col(\"created_at\").cast(TimestampType())) \\\n    .withColumnRenamed(\"created_at\", \"costumer_created_at\") \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"customer_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"consumer\")\n\nconsDF_ = spark.read.table(\"consumer\")\n\nrestDF \\\n    .withColumn(\"created_at\", col(\"created_at\").cast(TimestampType())) \\\n    .withColumnRenamed(\"id\", \"merchant_id\") \\\n    .withColumnRenamed(\"created_at\", \"merchant_created_at\") \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"merchant_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"restaurant\")\n\nrestDF_ = spark.read.table(\"restaurant\")\n\n# Traz todas as variáveis de consumer\njoin01 = temp00DF.join(consDF_, on=[\"customer_id\"], how=\"left\")\n\njoin01 \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"merchant_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"temp01\")\n\ntemp01DF = spark.read.table(\"temp01\")\n\n# Traz todas as variáveis de restaurant\njoin02 = temp01DF.join(restDF_, on=[\"merchant_id\"], how=\"left\")","user":"anonymous","dateUpdated":"2019-12-23T22:44:01+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577135879800_-1985364593","id":"20191223-140051_1115511627","dateCreated":"2019-12-23T21:17:59+0000","dateStarted":"2019-12-23T22:45:36+0000","dateFinished":"2019-12-23T22:47:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:177"},{"title":"Criptografando os dados sensíveis","text":"%pyspark\n\n# Criptografa os dados sensíveis\ncriptoDF = join02.select(criptografa_df(join02, cripto_ls))","user":"anonymous","dateUpdated":"2019-12-23T22:44:05+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577135879801_-180331052","id":"20191223-145213_1760585040","dateCreated":"2019-12-23T21:17:59+0000","dateStarted":"2019-12-23T22:47:12+0000","dateFinished":"2019-12-23T22:47:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:178"},{"title":"Gera a base final trusted orders","text":"%pyspark\n\n# Renomeia as colunas e cria as partições para gravação\ntrustedDF = criptoDF \\\n    .select(colunas) \\\n    .withColumn(\"dt_proc\", current_date()) \\\n    .withColumn(\"dt\", from_utc_timestamp(\"order_created_at\", col(\"merchant_timezone\"))) \\\n    .withColumn(\"dt\", col(\"dt\").cast(DateType())) \\\n    .repartition(\"dt\")\n\ntrustedDF.write.mode(\"overwrite\").partitionBy(\"dt\").option(\"compression\", \"snappy\").format(\"parquet\").save(destino_trusted)","user":"anonymous","dateUpdated":"2019-12-23T23:13:18+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577135879801_-783902458","id":"20191223-155858_1997045721","dateCreated":"2019-12-23T21:17:59+0000","dateStarted":"2019-12-23T23:13:18+0000","dateFinished":"2019-12-23T23:14:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:179"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2019-12-23T23:12:18+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1577142738450_1915573946","id":"20191223-231218_2053687780","dateCreated":"2019-12-23T23:12:18+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:180"}],"name":"ifood-trusted-order-dev","id":"2EXG7DCPQ","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}