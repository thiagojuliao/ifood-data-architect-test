{"paragraphs":[{"text":"%pyspark\n\n###############################################\n#   Importando pacotes & definindo funções    #\n###############################################\n\nfrom datetime import *\nfrom pyspark import StorageLevel\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# Função para criptografar colunas de um dataframe\ndef criptografa_df(dataframe, colunas_a_criptografar):\n    colunas = []\n    \n    for coluna in dataframe.columns:\n        if coluna in colunas_a_criptografar:\n            aux = sha2(col(coluna), 0).alias(coluna)\n        else:\n            aux = coluna\n        colunas.append(aux)\n    return colunas\n\n###############################\n#   Definição de variáveis    #\n###############################\n\n# Lista de colunas a serem criptografadas\ncripto_ls = [\"cpf\", \"customer_name\", \"delivery_address_city\", \"delivery_address_district\", \"delivery_address_country\", \"delivery_address_district\", \n    \"delivery_address_latitude\", \"delivery_address_longitude\", \"delivery_address_state\", \"delivery_address_zip_code\", \"customer_phone_area\", \n    \"customer_phone_number\", \"merchant_zip_code\"]\n\n# Ordem das colunas na base final\ncolunas = [\"order_id\", \"customer_id\", expr(\"cpf as customer_cpf\"), \"customer_name\", \"customer_phone_area\", \"customer_phone_number\", expr(\"language as customer_language\"), \n    \"costumer_created_at\", expr(\"active as is_costumer_active\"), \"merchant_id\", \"merchant_created_at\", expr(\"enabled as is_merchant_enabled\"), \n    expr(\"price_range as merchant_price_range\"), expr(\"average_ticket as merchant_average_ticket\"), expr(\"takeout_time as merchant_takeout_time\"), \n    expr(\"delivery_time as merchant_delivery_time\"), expr(\"minimum_order_value as merchant_minimum_order_value\"), \"merchant_zip_code\", \"merchant_city\",\n    \"merchant_state\", \"merchant_country\", \"merchant_latitude\", \"merchant_longitude\", \"merchant_timezone\", \"delivery_address_city\", \n    \"delivery_address_country\", \"delivery_address_district\", \"delivery_address_external_id\", \"delivery_address_latitude\", \"delivery_address_longitude\", \n    \"delivery_address_state\", \"delivery_address_zip_code\", expr(\"items as order_items\"), \"order_created_at\", expr(\"order_scheduled as is_order_scheduled\"),\n    \"order_scheduled_date\", \"order_total_amount\", \"origin_platform\", \"order_status\"]\n\n# Referências de processamento\nref = str(datetime.today() - timedelta(days=1))[0:10]\n\n# Caminho de origem da raw order\norigem_order = \"hdfs://localhost:9000/ifood-raw-order/dt={}/*.parquet\".format(ref)\n\n# Caminho de origem da raw order status\norigem_status = \"hdfs://localhost:9000/ifood-raw-order-status/dt={}/*.parquet\".format(ref)\n\n# Caminho de origem da raw restaurant\norigem_rest = \"hdfs://localhost:9000/ifood-raw-restaurant/\"\n\n# Caminho de origem da raw consumer\norigem_cons = \"hdfs://localhost:9000/ifood-raw-consumer/\"\n\n# Caminho de destino da trusted order\ndestino_trusted = \"hdfs://localhost:9000/ifood-trusted-order/\"\n\n# Inicia sessão spark\nspark = SparkSession.builder.appName(\"ifood-trusted-order-prod\").getOrCreate()\n\n# Configurações básicas para o spark\nspark.conf.set(\"spark.sql.maxPartitionBytes\", 200 * 1024 * 1024) # Seta a quantidade máxima do tamanho das partições ao ler os arquivos de entrada\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"DYNAMIC\") # Necessário para sobrescrever partições\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 100 * 1024 * 1024) # Define o limiar para broadcasting de até 100MB\nspark.conf.set(\"spark.default.parallelism\", 100) # Define a quantidade de tasks a serem executadas em paralelo","user":"anonymous","dateUpdated":"2019-12-25T12:59:11-0300","config":{"lineNumbers":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577281250472_778525068","id":"20191223-114810_835126986","dateCreated":"2019-12-25T10:40:50-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:514"},{"title":"Lê as origens order e order-status","text":"%pyspark\n\n# Lê a raw order\norderDF = spark.read.parquet(origem_order).drop(\"dt\", \"dt_proc\")\n\n# Lê a raw order status\nstatusDF = spark.read.parquet(origem_status).drop(\"dt\", \"dt_proc\")\n\n# Lê a raw restaurant\nrestDF = spark.read.parquet(origem_rest).drop(\"dt\", \"dt_proc\")\n\n# Lê a raw consumer\nconsDF = spark.read.parquet(origem_cons).drop(\"dt\", \"dt_proc\")","user":"anonymous","dateUpdated":"2019-12-25T12:59:30-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"> Volumetria da raw order: 151681\n> Volumetria da raw order status: 359703\n> Volumetria da raw restaurant: 7292\n> Volumetria da raw consumer: 809323\n"}]},"apps":[],"jobName":"paragraph_1577281250488_-305633364","id":"20191223-120737_447564544","dateCreated":"2019-12-25T10:40:50-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:515"},{"title":"Pegamos apenas os status mais recentes de cada pedido","text":"%pyspark\n\n# Status mais recentes de cada pedido\nstatusDF_ = statusDF \\\n    .withColumn(\"order\", expr(\"row_number() over(partition by order_id order by order_id, created_at desc)\")) \\\n    .filter(col(\"order\") == 1) \\\n    .select(\"order_id\", \"created_at\", expr(\"value as order_status\"))\n    \nstatusDF_.show(5, False)","user":"anonymous","dateUpdated":"2019-12-25T10:40:50-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------------------------------+------------------------+------------+\n|order_id                            |created_at              |order_status|\n+------------------------------------+------------------------+------------+\n|0002fbf3-5817-4eb3-9aec-bd4e6e346644|2019-01-31T16:35:20.000Z|CONCLUDED   |\n|0045528c-7367-47b6-8710-e595be901485|2019-01-30T04:00:07.000Z|CONCLUDED   |\n|009cb893-eebd-438e-80a4-c9d1f1f8d02b|2019-01-31T01:15:07.000Z|CONCLUDED   |\n|01048034-5f15-4dd1-8d01-2553679e9a46|2019-01-30T02:05:04.000Z|CONCLUDED   |\n|01050f76-2221-4ce2-b791-4b0c483de23b|2019-01-30T03:15:06.000Z|CONCLUDED   |\n+------------------------------------+------------------------+------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1577281250489_2071286004","id":"20191223-121509_717017178","dateCreated":"2019-12-25T10:40:50-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:516"},{"title":"Cruzando com a base orders","text":"%pyspark\n\n# Otimização do join\nspark.conf.set(\"spark.sql.shuffle.partitions\", int(spark.conf.get(\"spark.default.parallelism\")))\norderDF \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"order_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"orders\")\n    \norderDF_ = spark.read.table(\"orders\")\n\nstatusDF_ \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"order_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"status\")\n    \nstatusDF_ = spark.read.table(\"status\")\n    \n# Traz os status mais recentes de cada pedido\njoin00 = orderDF_.join(statusDF_, on=[\"order_id\"], how=\"left\")","user":"anonymous","dateUpdated":"2019-12-25T10:40:50-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577281250489_54414120","id":"20191223-122852_2071888743","dateCreated":"2019-12-25T10:40:50-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:517"},{"title":"Tratamento de duplicidade na base order","text":"%pyspark\n\n# Em desenvolvimento verificamos que a base order apresentava duplicidade devido a registros inválidos na data do pedido\n# Vamos tratá-los considerando todo registro que tiver na diferença created_at e order_created_at um valor menor ou igual a 1 como válido\njoin00_ = join00 \\\n    .withColumn(\"order_created_at\", col(\"order_created_at\").cast(TimestampType())) \\\n    .withColumn(\"created_at\", col(\"created_at\").cast(TimestampType())) \\\n    .withColumn(\"fl_valid\", expr(\"case when datediff(created_at, order_created_at) <= 1 then 1 else 0 end\")) \\\n    .filter(col(\"fl_valid\") == 1) \\\n    .drop(\"fl_valid\", \"created_at\")","user":"anonymous","dateUpdated":"2019-12-25T10:40:50-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577281250490_1076695880","id":"20191223-125627_1436993064","dateCreated":"2019-12-25T10:40:50-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:518"},{"title":"Cruzamento com as bases consumer e restaurant","text":"%pyspark\n\n# Otimização do join\njoin00_ \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"customer_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"temp00\")\n\ntemp00DF = spark.read.table(\"temp00\")\n\nconsDF \\\n    .drop(\"customer_name\") \\\n    .withColumn(\"created_at\", col(\"created_at\").cast(TimestampType())) \\\n    .withColumnRenamed(\"created_at\", \"costumer_created_at\") \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"customer_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"consumer\")\n\nconsDF_ = spark.read.table(\"consumer\")\n\nrestDF \\\n    .withColumn(\"created_at\", col(\"created_at\").cast(TimestampType())) \\\n    .withColumnRenamed(\"id\", \"merchant_id\") \\\n    .withColumnRenamed(\"created_at\", \"merchant_created_at\") \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"merchant_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"restaurant\")\n\nrestDF_ = spark.read.table(\"restaurant\")\n\n# Traz todas as variáveis de consumer\njoin01 = temp00DF.join(consDF_, on=[\"customer_id\"], how=\"left\")\n\njoin01 \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"merchant_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"temp01\")\n\ntemp01DF = spark.read.table(\"temp01\")\n\n# Traz todas as variáveis de restaurant\njoin02 = temp01DF.join(restDF_, on=[\"merchant_id\"], how=\"left\")","user":"anonymous","dateUpdated":"2019-12-25T10:40:50-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577281250491_-1352352908","id":"20191223-140051_1115511627","dateCreated":"2019-12-25T10:40:50-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:519"},{"title":"Criptografando os dados sensíveis","text":"%pyspark\n\n# Criptografa os dados sensíveis\ncriptoDF = join02.select(criptografa_df(join02, cripto_ls))","user":"anonymous","dateUpdated":"2019-12-25T10:40:50-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577281250491_-1694193002","id":"20191223-145213_1760585040","dateCreated":"2019-12-25T10:40:50-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:520"},{"title":"Gera a base final trusted orders","text":"%pyspark\n\n# Renomeia as colunas e cria as partições para gravação\ntrustedDF = criptoDF \\\n    .select(colunas) \\\n    .withColumn(\"dt_proc\", current_date()) \\\n    .withColumn(\"dt\", from_utc_timestamp(\"order_created_at\", col(\"merchant_timezone\"))) \\\n    .withColumn(\"dt\", col(\"dt\").cast(DateType())) \\\n    .repartition(\"dt\")\n\ntrustedDF.write.mode(\"overwrite\").partitionBy(\"dt\").option(\"compression\", \"snappy\").format(\"parquet\").save(destino_trusted)","user":"anonymous","dateUpdated":"2019-12-25T10:40:50-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577281250492_289258004","id":"20191223-155858_1997045721","dateCreated":"2019-12-25T10:40:50-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:521"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2019-12-25T10:40:50-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1577281250493_1664506285","id":"20191223-231218_2053687780","dateCreated":"2019-12-25T10:40:50-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:522"}],"name":"ifood-trusted-order-prod","id":"2EWUM7WRY","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}