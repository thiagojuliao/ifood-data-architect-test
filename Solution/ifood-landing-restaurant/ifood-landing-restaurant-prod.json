{"paragraphs":[{"title":"Inicialização dos pacotes, definindo funções e variáveis","text":"%pyspark\n\n###############################################\n#   Importando pacotes & definindo funções    #\n###############################################\n\nfrom datetime import *\nfrom pyspark import StorageLevel\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n###############################\n#   Definição de variáveis    #\n###############################\n\n# Referência de processamento do incremental\n#ref = str(datetime.today() - timedelta(days=1))[0:10]\nref = \"2017-01-23\"\n\n# Caminho de origem da pouso restaurant\norigem_pouso = \"s3://ifood-landing-restaurant/dt={}/*.csv\".format(ref)\n\n# Caminho de destino do incremental\ndestino_incremental = \"s3://ifood-raw-restaurant/\"\n\n# Inicia sessão spark\nspark = SparkSession.builder.appName(\"ifood-landing-restaurant-prod\").getOrCreate()\n\n# Configurações básicas para o spark\nspark.conf.set(\"spark.sql.maxPartitionBytes\", 200 * 1024 * 1024) # Seta a quantidade máxima de bytes em uma partição ao ler os arquivos de entrada (Entre 100MB e 200MB é o ideal)\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"DYNAMIC\") # Necessário para sobrescrever partições ","user":"anonymous","dateUpdated":"2019-12-21T16:13:03+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1576932860882_-547141070","id":"20191221-013414_540016259","dateCreated":"2019-12-21T12:54:20+0000","dateStarted":"2019-12-21T16:13:03+0000","dateFinished":"2019-12-21T16:13:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:54"},{"title":"Lê os dado origem","text":"%pyspark\n\n# Lê a pouso de origem\npousoDF = spark.read.option(\"header\", True).csv(origem_pouso)","user":"anonymous","dateUpdated":"2019-12-21T16:13:06+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1576932860884_-1632846196","id":"20191221-014425_1407235368","dateCreated":"2019-12-21T12:54:20+0000","dateStarted":"2019-12-21T16:13:06+0000","dateFinished":"2019-12-21T16:13:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:55"},{"title":"Cria e grava a tabela incremental","text":"%pyspark\n\n# Gera o incremental da tabela raw\nrawDF = pousoDF \\\n    .withColumn(\"dt_proc\", current_date()) \\\n    .withColumn(\"dt\", col(\"created_at\").cast(DateType()))\n\nrawDF.write.partitionBy(\"dt\").mode(\"overwrite\").option(\"compression\", \"snappy\").format(\"parquet\").save(destino_raw)","user":"anonymous","dateUpdated":"2019-12-21T16:13:20+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1576932860885_-603229231","id":"20191221-014717_699819168","dateCreated":"2019-12-21T12:54:20+0000","dateStarted":"2019-12-21T16:13:20+0000","dateFinished":"2019-12-21T16:13:21+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:56"},{"title":"Validação","text":"%pyspark\n\n# Leitura da base final\nrawDF_ = spark.read.parquet(destino_raw).filter(col(\"dt\") == lit(ref))\n\n# Validação Volumétrica\nprint(\"> Volumetria de saída equivale-se a de entrada ? --> {}\".format(pousoDF.count() == rawDF_.count()))\n\n# Validação do Schema\nschema = pousoDF.schema\nschema_ = rawDF_.drop(\"dt\", \"dt_proc\").schema\ncolunas = pousoDF.columns\ncolunas_ = rawDF_.drop(\"dt\", \"dt_proc\").columns\ncolunas.sort()\ncolunas_.sort()\nprint(\"> O schema de saída equivale-se ao de entrada ? ---> {}\".format(schema == schema_ or colunas == colunas_))","user":"anonymous","dateUpdated":"2019-12-21T16:13:42+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"> Volumetria de saída equivale-se a de entrada ? --> True\n> O schema de saída equivale-se ao de entrada ? ---> True\n"}]},"apps":[],"jobName":"paragraph_1576932860885_-1841648039","id":"20191221-015127_1384615969","dateCreated":"2019-12-21T12:54:20+0000","dateStarted":"2019-12-21T16:13:42+0000","dateFinished":"2019-12-21T16:13:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:57"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2019-12-21T16:13:34+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1576944814916_488211317","id":"20191221-161334_188685997","dateCreated":"2019-12-21T16:13:34+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:58"}],"name":"ifood-landing-restaurant-prod","id":"2EXQM4JV5","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}