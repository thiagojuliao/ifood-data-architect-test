{"paragraphs":[{"text":"%pyspark\n\n###############################################\n#   Importando pacotes & definindo funções    #\n###############################################\n\nfrom datetime import *\nfrom pyspark import StorageLevel\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\n# Função para criptografar colunas de um dataframe\ndef criptografa_df(dataframe, colunas_a_criptografar):\n    colunas = []\n    \n    for coluna in dataframe.columns:\n        if coluna in colunas_a_criptografar:\n            aux = sha2(col(coluna), 0).alias(coluna)\n        else:\n            aux = coluna\n        colunas.append(aux)\n    return colunas\n\n###############################\n#   Definição de variáveis    #\n###############################\n\n# Lista de colunas a serem criptografadas\ncripto_ls = [\"cpf\", \"customer_name\", \"delivery_address_city\", \"delivery_address_district\", \"delivery_address_country\", \"delivery_address_district\", \n    \"delivery_address_latitude\", \"delivery_address_longitude\", \"delivery_address_state\", \"delivery_address_zip_code\", \"customer_phone_area\", \n    \"customer_phone_number\", \"merchant_zip_code\"]\n\n# Ordem das colunas na base final\ncolunas = [\"order_id\", \"customer_id\", expr(\"cpf as customer_cpf\"), \"customer_name\", \"customer_phone_area\", \"customer_phone_number\", expr(\"language as customer_language\"), \n    \"costumer_created_at\", expr(\"active as is_costumer_active\"), \"merchant_id\", \"merchant_created_at\", expr(\"enabled as is_merchant_enabled\"), \n    expr(\"price_range as merchant_price_range\"), expr(\"average_ticket as merchant_average_ticket\"), expr(\"takeout_time as merchant_takeout_time\"), \n    expr(\"delivery_time as merchant_delivery_time\"), expr(\"minimum_order_value as merchant_minimum_order_value\"), \"merchant_zip_code\", \"merchant_city\",\n    \"merchant_state\", \"merchant_country\", \"merchant_latitude\", \"merchant_longitude\", \"merchant_timezone\", \"delivery_address_city\", \n    \"delivery_address_country\", \"delivery_address_district\", \"delivery_address_external_id\", \"delivery_address_latitude\", \"delivery_address_longitude\", \n    \"delivery_address_state\", \"delivery_address_zip_code\", expr(\"items as order_items\"), \"order_created_at\", expr(\"order_scheduled as is_order_scheduled\"),\n    \"order_scheduled_date\", \"order_total_amount\", \"origin_platform\", \"order_status\"]\n\n# Referências de processamento\n#ref00 = str(datetime.today() - timedelta(days=2))[0:10]\n#ref01 = str(datetime.today() - timedelta(days=1))[0:10]\nref00 = \"2019-01-30\"\nref01 = \"2019-01-31\"\n\n# Caminho de origem da raw order\norigem_order = \"s3://ifood-raw-order/\"\n\n# Caminho de origem da raw order status\norigem_status = \"s3://ifood-raw-order-status/\"\n\n# Caminho de origem da raw restaurant\norigem_rest = \"s3://ifood-raw-restaurant/\"\n\n# Caminho de origem da raw consumer\norigem_cons = \"s3://ifood-raw-consumer/\"\n\n# Caminho de destino da trusted order\ndestino_trusted = \"s3://ifood-trusted-order/\"\n\n# Inicia sessão spark\nspark = SparkSession.builder.appName(\"ifood-trusted-order-dev\").getOrCreate()\n\n# Configurações básicas para o spark\nspark.conf.set(\"spark.sql.maxPartitionBytes\", 200 * 1024 * 1024) # Seta a quantidade máxima de bytes em uma partição ao ler os arquivos de entrada (Entre 100MB e 200MB é o ideal)\nspark.conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"DYNAMIC\") # Necessário para sobrescrever partições\nspark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 100 * 1024 * 1024) # Define o limiar para broadcasting de até 100MB","user":"anonymous","dateUpdated":"2019-12-23T23:17:41+0000","config":{"lineNumbers":false,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577142986792_-926046664","id":"20191223-114810_835126986","dateCreated":"2019-12-23T23:16:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1090","dateFinished":"2019-12-23T23:17:41+0000","dateStarted":"2019-12-23T23:17:41+0000"},{"title":"Lê as origens order e order-status","text":"%pyspark\n\n# Lê a raw order\norderDF = spark.read.parquet(origem_order).filter(col(\"dt\").between(ref00, ref01)).drop(\"dt\", \"dt_proc\").persist(StorageLevel.DISK_ONLY)\nprint(\"> Volumetria da raw order: {}\".format(orderDF.count()))\n\n# Lê a raw order status\nstatusDF = spark.read.parquet(origem_status).filter(col(\"dt\").between(ref00, ref01)).drop(\"dt\", \"dt_proc\").persist(StorageLevel.DISK_ONLY)\nprint(\"> Volumetria da raw order status: {}\".format(statusDF.count()))\n\n# Lê a raw restaurant\nrestDF = spark.read.parquet(origem_rest).drop(\"dt\", \"dt_proc\").persist(StorageLevel.DISK_ONLY)\nprint(\"> Volumetria da raw restaurant: {}\".format(restDF.count()))\n\n# Lê a raw consumer\nconsDF = spark.read.parquet(origem_cons).drop(\"dt\", \"dt_proc\").persist(StorageLevel.DISK_ONLY)\nprint(\"> Volumetria da raw consumer: {}\".format(consDF.count()))","user":"anonymous","dateUpdated":"2019-12-23T23:18:07+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"> Volumetria da raw order: 151681\n> Volumetria da raw order status: 359703\n> Volumetria da raw restaurant: 7292\n> Volumetria da raw consumer: 809323\n"}]},"apps":[],"jobName":"paragraph_1577142986793_1398431390","id":"20191223-120737_447564544","dateCreated":"2019-12-23T23:16:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1091","dateFinished":"2019-12-23T23:18:17+0000","dateStarted":"2019-12-23T23:18:07+0000"},{"title":"Pegamos apenas os status mais recentes de cada pedido","text":"%pyspark\n\n# Status mais recentes de cada pedido\nstatusDF_ = statusDF \\\n    .withColumn(\"order\", expr(\"row_number() over(partition by order_id order by order_id, created_at desc)\")) \\\n    .filter(col(\"order\") == 1) \\\n    .select(\"order_id\", \"created_at\", expr(\"value as order_status\"))\n    \nstatusDF_.show(5, False)","user":"anonymous","dateUpdated":"2019-12-23T23:18:55+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------------------------------+------------------------+------------+\n|order_id                            |created_at              |order_status|\n+------------------------------------+------------------------+------------+\n|0002fbf3-5817-4eb3-9aec-bd4e6e346644|2019-01-31T16:35:20.000Z|CONCLUDED   |\n|0045528c-7367-47b6-8710-e595be901485|2019-01-30T04:00:07.000Z|CONCLUDED   |\n|009cb893-eebd-438e-80a4-c9d1f1f8d02b|2019-01-31T01:15:07.000Z|CONCLUDED   |\n|01048034-5f15-4dd1-8d01-2553679e9a46|2019-01-30T02:05:04.000Z|CONCLUDED   |\n|01050f76-2221-4ce2-b791-4b0c483de23b|2019-01-30T03:15:06.000Z|CONCLUDED   |\n+------------------------------------+------------------------+------------+\nonly showing top 5 rows\n\n"}]},"apps":[],"jobName":"paragraph_1577142986793_-2014693958","id":"20191223-121509_717017178","dateCreated":"2019-12-23T23:16:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1092","dateFinished":"2019-12-23T23:18:26+0000","dateStarted":"2019-12-23T23:18:26+0000"},{"title":"Cruzando com a base orders","text":"%pyspark\n\n# Otimização do join\nspark.conf.set(\"spark.sql.shuffle.partitions\", int(spark.conf.get(\"spark.default.parallelism\")))\norderDF \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"order_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"orders\")\n    \norderDF_ = spark.read.table(\"orders\")\n\nstatusDF_ \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"order_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"status\")\n    \nstatusDF_ = spark.read.table(\"status\")\n    \n# Traz os status mais recentes de cada pedido\njoin00 = orderDF_.join(statusDF_, on=[\"order_id\"], how=\"left\")","user":"anonymous","dateUpdated":"2019-12-23T23:18:31+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577142986793_-350979933","id":"20191223-122852_2071888743","dateCreated":"2019-12-23T23:16:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1093","dateFinished":"2019-12-23T23:18:42+0000","dateStarted":"2019-12-23T23:18:31+0000"},{"title":"Tratamento de duplicidade na base order","text":"%pyspark\n\n# Em desenvolvimento verificamos que a base order apresentava duplicidade devido a registros inválidos na data do pedido\n# Vamos tratá-los considerando todo registro que tiver na diferença created_at e order_created_at um valor menor ou igual a 1 como válido\njoin00_ = join00 \\\n    .withColumn(\"order_created_at\", col(\"order_created_at\").cast(TimestampType())) \\\n    .withColumn(\"created_at\", col(\"created_at\").cast(TimestampType())) \\\n    .withColumn(\"fl_valid\", expr(\"case when datediff(created_at, order_created_at) <= 1 then 1 else 0 end\")) \\\n    .filter(col(\"fl_valid\") == 1) \\\n    .drop(\"fl_valid\", \"created_at\")","user":"anonymous","dateUpdated":"2019-12-23T23:18:35+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577142986793_-865002965","id":"20191223-125627_1436993064","dateCreated":"2019-12-23T23:16:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1094","dateFinished":"2019-12-23T23:18:42+0000","dateStarted":"2019-12-23T23:18:42+0000"},{"title":"Cruzamento com as bases consumer e restaurant","text":"%pyspark\n\n# Otimização do join\njoin00_ \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"customer_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"temp00\")\n\ntemp00DF = spark.read.table(\"temp00\")\n\nconsDF \\\n    .drop(\"customer_name\") \\\n    .withColumn(\"created_at\", col(\"created_at\").cast(TimestampType())) \\\n    .withColumnRenamed(\"created_at\", \"costumer_created_at\") \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"customer_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"consumer\")\n\nconsDF_ = spark.read.table(\"consumer\")\n\nrestDF \\\n    .withColumn(\"created_at\", col(\"created_at\").cast(TimestampType())) \\\n    .withColumnRenamed(\"id\", \"merchant_id\") \\\n    .withColumnRenamed(\"created_at\", \"merchant_created_at\") \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"merchant_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"restaurant\")\n\nrestDF_ = spark.read.table(\"restaurant\")\n\n# Traz todas as variáveis de consumer\njoin01 = temp00DF.join(consDF_, on=[\"customer_id\"], how=\"left\")\n\njoin01 \\\n    .write.mode(\"overwrite\") \\\n    .bucketBy(int(spark.conf.get(\"spark.default.parallelism\")), \"merchant_id\") \\\n    .option(\"compression\", \"snappy\") \\\n    .format(\"parquet\") \\\n    .saveAsTable(\"temp01\")\n\ntemp01DF = spark.read.table(\"temp01\")\n\n# Traz todas as variáveis de restaurant\njoin02 = temp01DF.join(restDF_, on=[\"merchant_id\"], how=\"left\")","user":"anonymous","dateUpdated":"2019-12-23T23:18:37+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577142986793_-645051121","id":"20191223-140051_1115511627","dateCreated":"2019-12-23T23:16:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1095","dateFinished":"2019-12-23T23:19:37+0000","dateStarted":"2019-12-23T23:18:42+0000"},{"title":"Criptografando os dados sensíveis","text":"%pyspark\n\n# Criptografa os dados sensíveis\ncriptoDF = join02.select(criptografa_df(join02, cripto_ls))","user":"anonymous","dateUpdated":"2019-12-23T23:18:40+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577142986794_-181268424","id":"20191223-145213_1760585040","dateCreated":"2019-12-23T23:16:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1096","dateFinished":"2019-12-23T23:19:38+0000","dateStarted":"2019-12-23T23:19:37+0000"},{"title":"Gera a base final trusted orders","text":"%pyspark\n\n# Renomeia as colunas e cria as partições para gravação\ntrustedDF = criptoDF \\\n    .select(colunas) \\\n    .withColumn(\"dt_proc\", current_date()) \\\n    .withColumn(\"dt\", from_utc_timestamp(\"order_created_at\", col(\"merchant_timezone\"))) \\\n    .withColumn(\"dt\", col(\"dt\").cast(DateType())) \\\n    .repartition(\"dt\")\n\ntrustedDF.write.mode(\"overwrite\").partitionBy(\"dt\").option(\"compression\", \"snappy\").format(\"parquet\").save(destino_trusted)","user":"anonymous","dateUpdated":"2019-12-23T23:19:51+0000","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1577142986794_-2072858430","id":"20191223-155858_1997045721","dateCreated":"2019-12-23T23:16:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1097","dateFinished":"2019-12-23T23:19:59+0000","dateStarted":"2019-12-23T23:19:52+0000"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2019-12-23T23:16:26+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1577142986794_-1322998723","id":"20191223-231218_2053687780","dateCreated":"2019-12-23T23:16:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1098"}],"name":"ifood-trusted-order-prod","id":"2EWJEX6TN","noteParams":{},"noteForms":{},"angularObjects":{"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}